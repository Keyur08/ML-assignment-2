import streamlit as st
import pandas as pd
import numpy as np
import joblib
import plotly.graph_objects as go
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Page config
st.set_page_config(page_title="Loan Default Predictor", layout="wide", page_icon="üè¶")

# Custom CSS for unique styling
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        font-weight: 700;
        margin-bottom: 1rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
    </style>
""", unsafe_allow_html=True)

@st.cache_resource
def load_trained_models():
    model_paths = {
        'Logistic Regression': 'model/saved_models/logistic_regression.pkl',
        'Decision Tree': 'model/saved_models/decision_tree.pkl',
        'K-Nearest Neighbors': 'model/saved_models/knn.pkl',
        'Naive Bayes': 'model/saved_models/naive_bayes.pkl',
        'Random Forest': 'model/saved_models/random_forest.pkl',
        'XGBoost': 'model/saved_models/xgboost.pkl'
    }
    
    loaded_models = {}
    for name, path in model_paths.items():
        loaded_models[name] = joblib.load(path)
    
    scaler = joblib.load('model/saved_models/scaler.pkl')
    return loaded_models, scaler

def generate_confusion_heatmap(cm, model_name):
    fig, ax = plt.subplots(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
    ax.set_title(f'{model_name} - Confusion Matrix')
    ax.set_ylabel('Actual')
    ax.set_xlabel('Predicted')
    return fig

def main():
    st.markdown('<p class="main-header">üè¶ Vehicle Loan Default Prediction System</p>', unsafe_allow_html=True)
    
    models_dict, preprocessing_scaler = load_trained_models()
    
    # Sidebar
    st.sidebar.title("‚öôÔ∏è Configuration Panel")
    selected_model = st.sidebar.selectbox(
        "Choose Classification Model:",
        list(models_dict.keys()),
        index=0
    )
    
    st.sidebar.markdown("---")
    st.sidebar.info("üí° Upload test dataset (CSV format) to evaluate model performance")
    
    # Main content
    tab1, tab2 = st.tabs(["üìä Model Evaluation", "üìà Performance Metrics"])
    
    with tab1:
        st.subheader("Upload Test Dataset")
        uploaded_csv = st.file_uploader("Choose CSV file", type=['csv'], key="csv_uploader")
        
        if uploaded_csv is not None:
            test_df = pd.read_csv(uploaded_csv)
            st.success(f"‚úÖ Dataset loaded: {test_df.shape[0]} rows, {test_df.shape[1]} columns")
            
            with st.expander("üìã View Dataset Preview"):
                st.dataframe(test_df.head(10))
            
            
            if st.button("üöÄ Run Prediction Analysis", type="primary"):
                with st.spinner("Processing predictions..."):
                    # Load preprocessing artifacts
                    encoders = joblib.load('model/saved_models/encoders.pkl')
                    
                    # Separate features and target
                    if 'loan_default' in test_df.columns:
                        y_test = test_df['loan_default']
                        test_df_copy = test_df.drop(columns=['loan_default'])
                        has_labels = True
                    else:
                        test_df_copy = test_df.copy()
                        has_labels = False

                    from model.data_preprocessing import LoanDataHandler
                    processor = LoanDataHandler.__new__(LoanDataHandler)
                    processor.raw_data = test_df_copy
                    processor.label_encoders = encoders
                    processor.scaler = preprocessing_scaler
                    
                    processor.handle_missing_vals()
                    processor.drop_unnecessary()
                    processor.encode_categories()
                    
                    X_test = processor.raw_data
                    X_test_scaled = preprocessing_scaler.transform(X_test)
                    
                    # Predict
                    selected_clf = models_dict[selected_model]
                    predictions = selected_clf.predict(X_test_scaled)
                    
                    if hasattr(selected_clf, 'predict_proba'):
                        pred_probs = selected_clf.predict_proba(X_test_scaled)[:, 1]
                    else:
                        pred_probs = None
                    
                    # Display results
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        default_count = np.sum(predictions == 1)
                        st.metric("üî¥ Predicted Defaults", default_count)
                    
                    with col2:
                        safe_count = np.sum(predictions == 0)
                        st.metric("üü¢ Predicted Safe", safe_count)
                    
                    with col3:
                        default_rate = (default_count / len(predictions)) * 100
                        st.metric("üìä Default Rate", f"{default_rate:.2f}%")
                    
                    # Show metrics if labels available
                    if has_labels:
                        st.markdown("---")
                        st.subheader("üìâ Model Performance Metrics")
                        
                        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
                        
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            acc = accuracy_score(y_test, predictions)
                            st.metric("Accuracy", f"{acc:.4f}")
                        
                        with col2:
                            prec = precision_score(y_test, predictions, zero_division=0)
                            st.metric("Precision", f"{prec:.4f}")
                        
                        with col3:
                            rec = recall_score(y_test, predictions, zero_division=0)
                            st.metric("Recall", f"{rec:.4f}")
                        
                        with col4:
                            f1 = f1_score(y_test, predictions, zero_division=0)
                            st.metric("F1-Score", f"{f1:.4f}")
                        
                        # Confusion Matrix
                        st.markdown("---")
                        st.subheader("üîç Confusion Matrix Analysis")
                        
                        cm = confusion_matrix(y_test, predictions)
                        fig = generate_confusion_heatmap(cm, selected_model)
                        st.pyplot(fig)
                        
                        # Classification Report
                        with st.expander("üìë Detailed Classification Report"):
                            report = classification_report(y_test, predictions, output_dict=True)
                            st.dataframe(pd.DataFrame(report).transpose())
    
    with tab2:
        st.subheader("üìä Comparative Model Performance")
        
        metrics_df = pd.read_csv('model/saved_models/metrics_report.csv', index_col=0)
        
        st.dataframe(metrics_df.style.highlight_max(axis=0, color='lightgreen'))
        
        # Visual comparison
        st.markdown("---")
        st.subheader("üìà Metrics Visualization")
        
        metric_choice = st.selectbox("Select metric to visualize:", 
                                      ['accuracy', 'auc', 'precision', 'recall', 'f1', 'mcc'])
        
        fig = go.Figure(data=[
            go.Bar(x=metrics_df.index, y=metrics_df[metric_choice], 
                   marker_color='indianred')
        ])
        fig.update_layout(title=f'{metric_choice.upper()} Comparison Across Models',
                         xaxis_title='Model', yaxis_title=metric_choice.upper())
        st.plotly_chart(fig, use_container_width=True)

if __name__ == "__main__":
    main()